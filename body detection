  class FFSNNNetwork:
  
  def __init__(self, n_inputs, hidden_sizes=[2]):
    self.nx = n_inputs
    self.ny = 1
    self.nh = len(hidden_sizes)
    self.sizes = [self.nx] + hidden_sizes + [self.ny]
    
    self.W = {}
    self.B = {}
    for i in range(self.nh+1):
      self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])
      self.B[i+1] = np.zeros((1, self.sizes[i+1]))
  
  def sigmoid(self, x):
    return 1.0/(1.0 + np.exp(-x))
  
  def forward_pass(self, X):
    self.A = {}
    self.H = {}
    self.H[0] = X  #  (N , 2)
    for i in range(self.nh+1):
      self.A[i+1] = np.matmul(self.H[i], self.W[i+1]) + self.B[i+1] #  (N , 3)
      #print(self.A[i+1].shape)
      self.H[i+1] = self.sigmoid(self.A[i+1])  #  (N , 3)
      #print(self.H[i+1].shape)
    return self.H[self.nh+1]
  
  def grad_sigmoid(self, x):
    return x*(1-x) 
    
  def grad(self, X, Y):
    self.forward_pass(X)
    self.dW = {}
    self.dB = {}
    self.dH = {}
    self.dA = {}
    L = self.nh + 1
    
    self.dA[L] = (self.H[L] - Y.reshape(Y.shape[0],1))
    #print(self.dA[L].shape)
    for k in range(L, 0, -1):
      self.dW[k] = np.matmul(self.H[k-1].T, self.dA[k])
      #print(self.dW[k].shape)
      self.dB[k] = np.sum(self.dA[k], axis=0).reshape(1, -1)
      #print(self.dB[k].shape)
      self.dH[k-1] = np.matmul(self.dA[k], self.W[k].T)
      #print(self.dH[k-1].shape)
      self.dA[k-1] = np.multiply(self.dH[k-1], self.grad_sigmoid(self.H[k-1]))
      #print(self.dA[k-1].shape)

  def fit(self, X, Y, epochs=1, learning_rate=1, initialise=True, display_loss=False):
    
    
    if initialise:
      for i in range(self.nh+1):
        self.W[i+1] = np.random.randn(self.sizes[i], self.sizes[i+1])
        self.B[i+1] = np.zeros((1, self.sizes[i+1]))
        #print('Dim of W,B',i-1,'is',self.W[i+1].shape,self.B[i+1].shape)  
    if display_loss:
      loss = {}
    
    for e in tqdm_notebook(range(epochs), total=epochs, unit="epoch"):
      dW = {}
      dB = {}
      for i in range(self.nh+1):
        dW[i+1] = np.zeros((self.sizes[i], self.sizes[i+1]))
        dB[i+1] = np.zeros((1, self.sizes[i+1]))
        #print('Dim of dW,dB',i-1,'is',dW[i+1].shape,dB[i+1].shape)  
    
      self.grad(X, Y)
      for i in range(self.nh+1):
        dW[i+1] += self.dW[i+1]
        dB[i+1] += self.dB[i+1]
        
      m = X.shape[1]
      for i in range(self.nh+1):
        self.W[i+1] -= learning_rate * dW[i+1] / m
        self.B[i+1] -= learning_rate * dB[i+1] / m
      
      if display_loss:
        Y_pred = self.predict(X)
        loss[e] = mean_squared_error(Y_pred, Y)
    
    if display_loss:
      plt.plot(loss.values())
      plt.xlabel('Epochs')
      plt.ylabel('Mean Squared Error')
      plt.show()
      
  def predict(self, X):
    Y_pred = []
    for x in X:
      y_pred = self.forward_pass(x)
      Y_pred.append(y_pred)
    return np.array(Y_pred).squeeze()
